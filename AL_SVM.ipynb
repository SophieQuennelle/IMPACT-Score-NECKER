{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOUTES LES COMPLICATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "# pour gérer les path sous windows\n",
    "\n",
    "import pathlib\n",
    "import os \n",
    "import shutil\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import glob\n",
    "\n",
    "# Pour le modèle \n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from time import time\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import itertools\n",
    "import shutil\n",
    "from sklearn import preprocessing\n",
    "from  sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.utils import shuffle\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETAPE 1 = PREPOCESSING : à ne faire qu'une fois, si les dossiers ont déjà été créés et qu'on a déjà commencé l'apprentissage il faut passer directement à l'étape 2."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Si on a déjà lancer le modele une fois et qu'on veut recommencer :\n",
    "## il faut supprimer les fichier dev train et unlabeled\n",
    "\n",
    "path = os.getcwd()\n",
    "\n",
    "shutil.rmtree(\"train\", ignore_errors=True)\n",
    "\n",
    "shutil.rmtree(\"dev\", ignore_errors=True)\n",
    "\n",
    "shutil.rmtree(\"unlabeled\", ignore_errors=True)\n",
    "\n",
    "shutil.rmtree(\"all_clac\", ignore_errors=True)\n",
    "\n",
    "shutil.rmtree(\"all_pas_clac\", ignore_errors=True)\n",
    "\n",
    "## il faut aussi supprimé les fichiers excel test, df et table de correspondance\n",
    "\n",
    "os.remove(\"./test_unlabeled.xlsx\")\n",
    "os.remove(\"./table_de_correspondance.xlsx\")\n",
    "os.remove(\"./df_stacked.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Créer les dossiers avec l'architecture requise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, dev et unlabeled\n",
    "\n",
    "path = \"train/clac\"\n",
    "os.makedirs(path)\n",
    "\n",
    "path = os.getcwd()\n",
    "\n",
    "path = \"train/pas_clac\"\n",
    "os.makedirs(path)\n",
    "\n",
    "path = \"dev/clac\"\n",
    "os.makedirs(path)\n",
    "\n",
    "path = \"dev/pas_clac\"\n",
    "os.makedirs(path)\n",
    "\n",
    "path = \"unlabeled/unlabeled\"\n",
    "os.makedirs(path)\n",
    "\n",
    "\n",
    "# all_clac et all_pas_clac pour le shuffle\n",
    "\n",
    "path_all_clac = \"./all_clac\"\n",
    "os.makedirs(path_all_clac)\n",
    "\n",
    "path_all_pas_clac = \"./all_pas_clac\"\n",
    "os.makedirs(path_all_pas_clac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Il faut mettre quelques exemples de clac et de pas clac annotés à la main dans les fichiers dev et train \n",
    "avant de commencer. On peut utiliser les exemples rangés dans les dossiers exemple_de_clac et exemple_de_pas_clac.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) CHARGER EXTRACTION TOUTES LES COMPLICATIONS\n",
    "# Il faut que la première ligne soit IPP DDK COMPLICATION1 COMPLICATIONX\n",
    "\n",
    "extraction_tc = pd.read_excel('extraction_tc.xlsx')\n",
    "\n",
    "df_extraction_tc = pd.DataFrame(extraction_tc)\n",
    "\n",
    "df_stacked1 = df_extraction_tc.set_index(['IPP', 'DDK']).stack()\n",
    "\n",
    "extraction_tc2 = pd.read_excel('extraction_tc2.xlsx')\n",
    "\n",
    "df_extraction_tc2 = pd.DataFrame(extraction_tc2)\n",
    "\n",
    "df_stacked2 = df_extraction_tc2.set_index(['IPP', 'DDK']).stack()\n",
    "\n",
    "extraction_tc3 = pd.read_excel('extraction_tc3.xlsx')\n",
    "\n",
    "df_extraction_tc3 = pd.DataFrame(extraction_tc3)\n",
    "\n",
    "df_stacked3 = df_extraction_tc3.set_index(['IPP', 'DDK']).stack()\n",
    "\n",
    "df_stacked = pd.concat([df_stacked1, df_stacked2, df_stacked3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Pour créer une TABLE DE CORRESPONDANCE\n",
    "\n",
    "df_stacked = df_stacked.reset_index()\n",
    "\n",
    "df_stacked.rename(columns={0:'texte_complication', 'level_2':'type_complication'}, inplace=True)\n",
    "\n",
    "df_stacked.to_excel('table_de_correspondance.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) PREPROCESSING du texte maintenant pour qu'il s'applique à tous les exemples\n",
    "# forcer texte_complication à être du str et créer df_stacked.xlsx\n",
    "df_stacked = df_stacked.astype({'texte_complication': str}).dropna(subset=['texte_complication'])\n",
    "df_stacked.to_excel('df_stacked.xlsx')\n",
    "\n",
    "# Pour enlever la ponctuaction et les mots vides de sens\n",
    "# note pour plus tard : enlever les mots rares\n",
    "\n",
    "noise_list = [\",\", \".\", \"?\", \";\", \":\", \"/\", \"!\", \"-\", \"+\", \"ÿ\", \"à\"] \n",
    "word_noise_list = [\"a\", \"ainsi\", \"assez\", \"au\", \"aux\", \"ce\", \"ceci\", \"cela\",\n",
    "                   \"car\", \"ces\", \"cette\", \"ce\", \"celle\", \"du\", \"en\", \"il\", \"ils\"\n",
    "                   \"elle\", \"elles\", \"que\", \"qui\", \"qu'\", \"se\", \"son\", \"sa\", \"ses\",\n",
    "                   \"le\", \"la\", \"les\", \"l'\", \"un\", \"une\", \"de\", \"des\", \"au\", \"du\"]\n",
    "\n",
    "\n",
    "def remove_noise(input_text):\n",
    "    noise_free_words = [caracter for caracter in input_text.lower() if caracter not in noise_list]\n",
    "    noise_free_text = \"\".join(noise_free_words) \n",
    "    word_list = [word for word in noise_free_text.split(\" \") if word not in word_noise_list]\n",
    "    ouput_text = \" \".join(word_list)\n",
    "    return ouput_text\n",
    "\n",
    "df_stacked['texte_complication'] = df_stacked['texte_complication'].apply(remove_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il faut mettre de coté : 1771 exemples.\n"
     ]
    }
   ],
   "source": [
    "# 4) SPLIT EXPORT EN DEV ET MODEL SETS\n",
    "# Mettre 10% des exemples au hasard de coté, il faudra les labeliser à la main après\n",
    "# Pour avoir à la fin un test set representatif de la population \n",
    "# (Et pas les 1000 derniers KT pour le test set)\n",
    "\n",
    "# Pour savoir combien d'exemples ils faut mettre de côté\n",
    "df_stacked\n",
    "index = df_stacked.index\n",
    "number_of_rows = len(index)\n",
    "x = number_of_rows // 10\n",
    "print(\"Il faut mettre de coté :\", x, \"exemples.\")\n",
    "\n",
    "# Pour mettre de côté, dans un xlsx séparé, ces x exemples\n",
    "df_test = df_stacked.sample(n=x, random_state=1)\n",
    "df_test.to_excel(\"test_unlabeled.xlsx\") \n",
    "\n",
    "# Les colonnes qui ne sont pas communes aux deux df (donc l'extraction - le test) vont servir pour le model\n",
    "df_model = pd.concat([df_stacked, df_test]).drop_duplicates(keep=False)\n",
    "\n",
    "#### Pour résumer on a crée :\n",
    "\n",
    "# un fichier excel avec 10% des exemples qu'il faudra fusionner avec les autres extractions \n",
    "# + labeliser à la main pour faire un test  set. \n",
    "# avec les autres exemples : une df \"df_model\" à partir de laquelle il faut sortir un dev set et un training set\n",
    "# on va les labeliser là"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour enlever les exemples qui sont déjà dans le dev, le test et le train set\n",
    "\n",
    "test_id_list = list(pd.read_excel(\"test_unlabeled.xlsx\", index_col=0).index)\n",
    "\n",
    "labeled_files = []\n",
    "for directory in ['train/clac', 'train/pas_clac', 'dev/clac', 'dev/pas_clac' ]:\n",
    "    labeled_files += [str(f[:-4]) for f in listdir(directory) if f != 'desktop.ini' if isfile(join(directory, f))]\n",
    "print(labeled_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enlever de df_stacked, les exemples dans test ou labelises \n",
    "\n",
    "all_indices_to_removes = test_id_list + labeled_files\n",
    "\n",
    "indices_to_keep = df_stacked.index.difference(all_indices_to_removes)\n",
    "df_stacked = df_stacked.loc[indices_to_keep]\n",
    "df_stacked.to_excel('df_stacked.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) MODEL -> DOSSIER UNLABELED\n",
    "# création, dans le dossier unlabeled de fichier csv contenant l'exemple et nommé d'après l'index de la \n",
    "# table de correspondance\n",
    "\n",
    "directory = pathlib.Path().absolute()\n",
    "DATA_FOLDER = directory\n",
    "TRAIN_FOLDER = os.path.join(DATA_FOLDER, \"train\")\n",
    "DEV_FOLDER = os.path.join(DATA_FOLDER, \"dev\")\n",
    "UNLABELED_FOLDER = os.path.join(DATA_FOLDER, \"unlabeled\")\n",
    "ENCODING = 'utf-8'\n",
    "categories = ['clac', 'pas_clac']\n",
    "\n",
    "for idx, row_data in df_stacked.iterrows():\n",
    "    file_name = os.path.join(UNLABELED_FOLDER, 'unlabeled')\n",
    "    file_name = os.path.join(os.path.join(file_name), str(idx) + '.txt')\n",
    "       \n",
    "    create_file = open(file_name, \"w+\", encoding='utf-8')\n",
    "    create_file.write(row_data['texte_complication'])\n",
    "    create_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETAPE 2 : Entraîner le modèle"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Le premier tour est long mais après ça va vite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version SANS balance des classes\n",
    "\n",
    "directory = pathlib.Path().absolute()\n",
    "\n",
    "NUM_QUESTIONS = 1 #nombre d'exemple qu'il propose à chaque tour\n",
    "PLOT_RESULTS = True\n",
    "ACTIVE = True\n",
    "DATA_FOLDER = directory\n",
    "TRAIN_FOLDER = os.path.join(DATA_FOLDER, \"train\") \n",
    "DEV_FOLDER = os.path.join(DATA_FOLDER, \"dev\") \n",
    "UNLABELED_FOLDER = os.path.join(DATA_FOLDER, \"unlabeled\")\n",
    "ENCODING = 'utf-8'\n",
    "while True:\n",
    "    data_train = load_files(TRAIN_FOLDER, encoding=ENCODING)\n",
    "    data_dev = load_files(DEV_FOLDER, encoding=ENCODING)\n",
    "    data_unlabeled = load_files(UNLABELED_FOLDER, encoding=ENCODING)\n",
    "    categories = data_train.target_names\n",
    "    \n",
    "    def size_mb(docs):\n",
    "        return sum(len(s.encode('utf-8')) for s in docs) / 1e6\n",
    "    \n",
    "    data_train_size_mb = size_mb(data_train.data)\n",
    "    data_dev_size_mb = size_mb(data_dev.data)\n",
    "    data_unlabeled_size_mb = size_mb(data_unlabeled.data)\n",
    "    \n",
    "    print(\"%d documents - %0.3fMB (training set)\" % (\n",
    "        len(data_train.data), data_train_size_mb))\n",
    "    print(\"%d documents - %0.3fMB (dev set)\" % (\n",
    "        len(data_dev.data), data_dev_size_mb))\n",
    "    print(\"%d documents - %0.3fMB (unlabeled set)\" % (\n",
    "        len(data_unlabeled.data), data_unlabeled_size_mb))\n",
    "    print(\"%d categories\" % len(categories))\n",
    "    print()\n",
    "    y_train = data_train.target\n",
    "    y_dev =  data_dev.target\n",
    "    \n",
    "    \n",
    "    print(\"Extracting features from the training dataset using a sparse vectorizer\")\n",
    "    t0 = time()\n",
    "    vectorizer = TfidfVectorizer(encoding= ENCODING, use_idf=True, norm='l1', binary=False, sublinear_tf=True, min_df=0.001, max_df=1.0, ngram_range=(1, 2), analyzer='word', stop_words=None)\n",
    "    \n",
    "    # the output of the fit_transform (x_train) is a sparse csc matrix.\n",
    "    X_train = vectorizer.fit_transform(data_train.data)\n",
    "    duration = time() - t0\n",
    "    # print(\"done in %fs at %0.3fMB/s\" % (duration, data_train_size_mb / duration))\n",
    "    print(\"n_samples: %d, n_features: %d\" % X_train.shape)\n",
    "    print()\n",
    "    \n",
    "    print(\"Extracting features from the dev dataset using the same vectorizer\")\n",
    "    t0 = time()\n",
    "    X_dev = vectorizer.transform(data_dev.data)\n",
    "    duration = time() - t0\n",
    "    # print(\"done in %fs at %0.3fMB/s\" % (duration, data_dev_size_mb / duration))\n",
    "    print(\"n_samples: %d, n_features: %d\" % X_dev.shape)\n",
    "    print()\n",
    "    \n",
    "    print(\"Extracting features from the unlabled dataset using the same vectorizer\")\n",
    "    t0 = time()\n",
    "    X_unlabeled = vectorizer.transform(data_unlabeled.data)\n",
    "    duration = time() - t0\n",
    "    # print(\"done in %fs at %0.3fMB/s\" % (duration, data_unlabeled_size_mb / duration))\n",
    "    print(\"n_samples: %d, n_features: %d\" % X_unlabeled.shape)\n",
    "    print()\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    def trim(s):\n",
    "        \"\"\"Trim string to fit on terminal (assuming 80-column display)\"\"\"\n",
    "        return s if len(s) <= 80 else s[:77] + \"...\"\n",
    "    \n",
    "    ###############################################################################\n",
    "    # Benchmark classifiers\n",
    "    def benchmark(clf, X_train, X_dev, y_train, y_dev, X_unlabeled):\n",
    "        print('_' * 80)\n",
    "        print(\"Training: \")\n",
    "        print(clf)\n",
    "        t0 = time()\n",
    "        \n",
    "        # Create a scaler fitted to X_train to later standarize all the subsets with the same scale ------------------\n",
    "        scaler = preprocessing.StandardScaler(with_mean=False)\n",
    "        scaler = scaler.fit(X_train)\n",
    "\n",
    "        X_train = scaler.transform(X_train)  # Standardizing ------------------     \n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        train_time = time() - t0\n",
    "        print(\"train time: %0.3fs\" % train_time)\n",
    "    \n",
    "        t0 = time()\n",
    "        X_dev = scaler.transform(X_dev) # Standardizing ------------------\n",
    "        pred = clf.predict(X_dev) \n",
    "        dev_time = time() - t0\n",
    "        print(\"dev time:  %0.3fs\" % dev_time)\n",
    "    \n",
    "        score = metrics.f1_score(y_dev, pred)\n",
    "        accscore = metrics.accuracy_score(y_dev, pred)\n",
    "        print (\"pred count is %d\" %len(pred))\n",
    "        print ('accuracy score:     %0.3f' % accscore)\n",
    "        print(\"f1-score:   %0.3f\" % score)\n",
    "    \n",
    "        if hasattr(clf, 'coef_'):\n",
    "            print(\"dimensionality: %d\" % clf.coef_.shape[1])\n",
    "            print(\"density: %f\" % density(clf.coef_))\n",
    "    \n",
    "            \n",
    "        \n",
    "        #print(\"classification report:\")\n",
    "        #print(metrics.classification_report(y_dev, pred,\n",
    "                                                #target_names=categories))\n",
    "    \n",
    "        \n",
    "        print(\"confusion matrix:\")\n",
    "        print(metrics.confusion_matrix(y_dev, pred))\n",
    "        tn, fp, fn, tp = confusion_matrix(y_dev, pred).ravel()\n",
    "        print(\"tn :\", tn, \"fp :\", fp, \"fn :\", fn, \"tp :\", tp)\n",
    "        \n",
    "        # Plot non-normalized confusion matrix\n",
    "        titles_options = [(\"Confusion matrix, without normalization\", None),\n",
    "                          (\"Normalized confusion matrix\", 'true')]\n",
    "        for title, normalize in titles_options:\n",
    "            disp = plot_confusion_matrix(clf, X_dev, y_dev,\n",
    "                                 display_labels=categories,\n",
    "                                 cmap=plt.cm.Blues,\n",
    "                                 normalize=normalize)\n",
    "            disp.ax_.set_title(title)\n",
    "\n",
    "            print(title)\n",
    "            print(disp.confusion_matrix)\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"confidence for unlabeled data:\")\n",
    "        \n",
    "        X_unlabeled = scaler.transform(X_unlabeled) # Standardizing ------------------\n",
    "        \n",
    "        #compute absolute confidence for each unlabeled sample in each class\n",
    "        confidences = np.abs(clf.decision_function(X_unlabeled))\n",
    "        #average abs(confidence) over all classes for each unlabeled sample (if there is more than 2 classes)\n",
    "        if(len(categories) > 2):\n",
    "            confidences = np.average(confidences, axix=1)\n",
    "        \n",
    "        print(confidences)\n",
    "        sorted_confidences = np.argsort(confidences)\n",
    "        question_samples = []\n",
    "        #select top k low confidence unlabeled samples\n",
    "        low_confidence_samples = sorted_confidences[0:NUM_QUESTIONS]\n",
    "        #select top k high confidence unlabeled samples\n",
    "        high_confidence_samples = sorted_confidences[-NUM_QUESTIONS:]\n",
    "\n",
    "        question_samples.extend(low_confidence_samples.tolist())\n",
    "        question_samples.extend(high_confidence_samples.tolist())\n",
    "\n",
    "        \n",
    "        print()\n",
    "        clf_descr = str(clf).split('(')[0]\n",
    "        return clf_descr, score, train_time, dev_time, question_samples\n",
    "    \n",
    "    \n",
    "    results = []\n",
    "    results.append(benchmark(LinearSVC(loss='l2', penalty='l2', \n",
    "                                                dual=False, tol=1e-3, class_weight='balanced'), \n",
    "                                                 X_train, X_dev, y_train, y_dev, X_unlabeled))\n",
    " \n",
    "    \n",
    "    # make some plots\n",
    "    \n",
    "    indices = np.arange(len(results))\n",
    "    \n",
    "    results = [[x[i] for x in results] for i in range(5)]\n",
    "    \n",
    "    clf_names, score, training_time, dev_time, question_samples = results\n",
    "    training_time = np.array(training_time) / np.max(training_time)\n",
    "    dev_time = np.array(dev_time) / np.max(dev_time)\n",
    "    \n",
    "    #if PLOT_RESULTS:\n",
    "        #pl.figure(figsize=(12,8))\n",
    "        #pl.title(\"Score\")\n",
    "        #pl.barh(indices, score, .2, label=\"score\", color='r')\n",
    "        #pl.barh(indices + .3, training_time, .2, label=\"training time\", color='g')\n",
    "        #pl.barh(indices + .6, dev_time, .2, label=\"dev time\", color='b')\n",
    "        #pl.yticks(())\n",
    "        #pl.legend(loc='best')\n",
    "        #pl.subplots_adjust(left=.25)\n",
    "        #pl.subplots_adjust(top=.95)\n",
    "        #pl.subplots_adjust(bottom=.05)\n",
    "        \n",
    "        #for i, c in zip(indices, clf_names):\n",
    "        #    pl.text(-.3, i, c)\n",
    "        #pl.savefig('ngramoptimize.png')\n",
    "        #pl.show()\n",
    "\n",
    "    if ACTIVE:\n",
    "        for i in question_samples[0]:\n",
    "            filename = data_unlabeled.filenames[i]\n",
    "            print (filename)\n",
    "            print ('**************************content***************************')\n",
    "            print (data_unlabeled.data[i])\n",
    "            print ('**************************content end***********************')\n",
    "            print (\"Annotate this text (select one label):\")\n",
    "            for i in range(0, len(categories)):\n",
    "                print (\"%d = %s\" %(i+1, categories[i]))\n",
    "            labelNumber = input(\"Enter the correct label number:\")\n",
    "            while labelNumber.isdigit()== False:\n",
    "                labelNumber = input(\"Enter the correct label number (a number please):\")\n",
    "            labelNumber = int(labelNumber)\n",
    "            category = categories[labelNumber - 1] \n",
    "            dstDir = os.path.join(TRAIN_FOLDER, category) \n",
    "            shutil.move(filename, dstDir)\n",
    "            \n",
    "        # shuffle pour mélanger les exemples dans dev et train à chaque cycle\n",
    "        # sinon on va avoir un modèle sur psécialiser dans la classification des clac initiales \n",
    "            \n",
    "            dstDir = os.path.join(\"./all_clac\") \n",
    "            srcDir = os.path.join(\"./train/clac/\")\n",
    "            # selectionner les fichiers texte à déplacer\n",
    "            files = [f for f in glob.glob(srcDir + \"/*.txt\", recursive=True)]\n",
    "\n",
    "            #for f in files:\n",
    "             #   print(f)\n",
    "\n",
    "            # déplacer les fichier txt\n",
    "            for f in files:\n",
    "                shutil.move(f, dstDir)\n",
    "                \n",
    "            # pour le dev\n",
    "            # définir le dossier source et le dossier destination\n",
    "            dstDir = os.path.join(\"./all_clac\") \n",
    "            srcDir2 = os.path.join(\"./dev/clac\")\n",
    "\n",
    "            # selectionner les fichiers texte à déplacer\n",
    "            files = [f for f in glob.glob(srcDir2 + \"/*.txt\", recursive=True)]\n",
    "\n",
    "            #for f in files:\n",
    "             #   print(f)\n",
    "\n",
    "            # déplacer les fichier txt\n",
    "            for f in files:\n",
    "                shutil.move(f, dstDir)\n",
    "            \n",
    "            #2) mettre tous les fichiers pas_clac de dev et de train dans le même dossier all_pas_clac \n",
    "            \n",
    "\n",
    "            # déplacer tous les fichier.txt des dossiers train/pas_clac et dev/pas_clac dans all_pas_clac\n",
    "\n",
    "            # pour le train\n",
    "            # définir le dossier source et le dossier destination\n",
    "            dstDir = os.path.join(\"./all_pas_clac\") \n",
    "            srcDir = os.path.join(\"./train/pas_clac/\")\n",
    "\n",
    "            # selectionner les fichiers texte à déplacer\n",
    "            files = [f for f in glob.glob(srcDir + \"/*.txt\", recursive=True)]\n",
    "\n",
    "            #for f in files:\n",
    "            #    print(f)\n",
    "\n",
    "            # déplacer les fichier txt\n",
    "            for f in files:\n",
    "                shutil.move(f, dstDir)\n",
    "    \n",
    "            # pour le dev\n",
    "            # définir le dossier source et le dossier destination\n",
    "            dstDir = os.path.join(\"./all_pas_clac\") \n",
    "            srcDir2 = os.path.join(\"./dev/pas_clac\")\n",
    "\n",
    "            # selectionner les fichiers texte à déplacer\n",
    "            files = [f for f in glob.glob(srcDir2 + \"/*.txt\", recursive=True)]\n",
    "\n",
    "            #for f in files:\n",
    "           #     print(f)\n",
    "\n",
    "            # déplacer les fichier txt\n",
    "            for f in files:\n",
    "                shutil.move(f, dstDir)\n",
    "            \n",
    "            srcDir_all_clac = os.path.join(\"./all_clac\")\n",
    "            srcDir_all_pas_clac = os.path.join(\"./all_pas_clac\")\n",
    "\n",
    "            # Nombre d'exemples qui sont bien des complications du cathéterisme\n",
    "           \n",
    "            no_clac = len(os.listdir(srcDir_all_clac))\n",
    "\n",
    "            print(\"Nombre de complications liées au cathéterisme :\")\n",
    "            print(no_clac)\n",
    "\n",
    "             # Nombre d'exemples qui ne sont pas des complications du cathéterisme\n",
    "\n",
    "            no_pas_clac = len(os.listdir(srcDir_all_pas_clac))\n",
    "\n",
    "            print(\"Nombre d'exemples qui ne sont pas des complications liées au cathéterisme :\")\n",
    "            print(no_pas_clac)\n",
    "\n",
    "\n",
    "            # Nombre total d'exemples \n",
    "\n",
    "            no_total = len( (os.listdir(srcDir_all_clac)) + (os.listdir(srcDir_all_pas_clac)) )\n",
    "\n",
    "            print(\"Total :\")  \n",
    "            print(no_total)\n",
    "            \n",
    "            s_dev_clac = int(no_clac * 0.3)\n",
    "            \n",
    "            print(\"Nombre d'exemples de clac pour le dev set :\")\n",
    "            print(s_dev_clac)\n",
    "\n",
    "            s_dev_pas_clac = int(no_pas_clac * 0.3)\n",
    "\n",
    "            print(\"Nombre d'exemples de pas clac pour le dev set :\")\n",
    "            print(s_dev_pas_clac)\n",
    "\n",
    "            s_train_clac = no_clac - s_dev_clac\n",
    "\n",
    "            print(\"Nombre d'exemples de clac pour le train set :\")\n",
    "            print(s_train_clac)\n",
    "\n",
    "            s_train_pas_clac = no_pas_clac - s_dev_pas_clac\n",
    "\n",
    "            print(\"Nombre d'exemples de pas clac pour le train set :\")\n",
    "            print(s_train_pas_clac)\n",
    "            \n",
    "            s_train = s_train_clac + s_train_pas_clac\n",
    "\n",
    "            print(\"Nombre d'exemples pour le train set :\")\n",
    "            print(s_train)\n",
    "\n",
    "            s_dev = s_dev_clac + s_dev_pas_clac\n",
    "\n",
    "            print(\"Nombre d'exemples pour le dev set :\")\n",
    "            print(s_dev)\n",
    "\n",
    "        \n",
    "            def move(srcDir, dstDir, share=None):\n",
    "                files = [f for f in glob.glob(srcDir + \"/*.txt\", recursive=True)]\n",
    "                if share is None:\n",
    "                    share =  len(files)\n",
    "                for f in random.sample(files, share):\n",
    "                    shutil.move(f, dstDir)\n",
    "                    \n",
    "            # Pour le dossier dev_clac\n",
    "\n",
    "            srcDir = os.path.join(\"./all_clac\")\n",
    "            dstDir = os.path.join(\"./dev/clac\")\n",
    "            share = s_dev_clac\n",
    "\n",
    "            move(srcDir, dstDir, share)\n",
    "            \n",
    "            # Pour train_clac\n",
    "\n",
    "            srcDir = os.path.join(\"./all_clac\")\n",
    "            dstDir = os.path.join(\"./train/clac\")\n",
    "            share = (s_train_clac - 1) # le - 1 là c'est au cas où all_clac soit impair (du coup il n'y a pas assez d'exemple)\n",
    "\n",
    "            move(srcDir, dstDir, share)\n",
    "\n",
    "\n",
    "            # Pour dev_pas_clac\n",
    "\n",
    "            srcDir = os.path.join(\"./all_pas_clac\")\n",
    "            dstDir = os.path.join(\"./dev/pas_clac\")\n",
    "            share = s_dev_pas_clac\n",
    "\n",
    "            move(srcDir, dstDir, share)\n",
    "            \n",
    "            # Pour train_pas_clac\n",
    "\n",
    "            srcDir = os.path.join(\"./all_pas_clac\")\n",
    "            dstDir = os.path.join(\"./train/pas_clac\")\n",
    "            share = (s_train_pas_clac - 1) # le - 1 là c'est au cas où all_clac soit impair (du coup il n'y a pas assez d'exemple)\n",
    "\n",
    "            move(srcDir, dstDir, share)\n",
    "            \n",
    "            \n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# ETAPE 3 : Pour classifier tous les unlabeled, une fois l'algo entraîné\n",
    "\n",
    "all_labels = results[0][0].predict(X_unlabeled)\n",
    "for it in range(len(all_labels)):\n",
    "    \n",
    "    lab = all_labels[it]\n",
    "    filename = data_unlabeled.filenames[it]\n",
    "    \n",
    "    category = categories[lab - 1]\n",
    "    dstDir = os.path.join(TRAIN_FOLDER, category) \n",
    "    shutil.move(filename, dstDir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
