{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding des sequences avec CamamBERT puis classification par le TextClassifier de FLAIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import flair\n",
    "from flair.data import Sentence\n",
    "from flair.embeddings import CamembertEmbeddings\n",
    "from flair.embeddings import TransformerWordEmbeddings\n",
    "import pathlib\n",
    "import os\n",
    "from flair.data import Corpus\n",
    "from flair.datasets import CSVClassificationCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les jeux de données : test = jeu d'évaluation final pour flair, \n",
    "# Annoté sans active learning, parce que l'AL pourrait entraîner un biais.\n",
    "\n",
    "train_set = pd.read_csv('train.csv', sep=';', encoding='utf-8')\n",
    "dev_set = pd.read_csv('dev.csv', sep=';', encoding='utf-8')\n",
    "test_set = pd.read_csv('test.csv', sep=';', encoding='utf-8')\n",
    "\n",
    "train_set = train_set[['index', 'exemple', 'label']].set_index('index')\n",
    "test_set = test_set[['index', 'exemple', 'label']].set_index('index')\n",
    "dev_set = dev_set[['index', 'exemple', 'label']].set_index('index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophie/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated method __init__. (Use 'TransformerWordEmbeddings' for all transformer-based word embeddings) -- Deprecated since version 0.4.5.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# Embedding des exemples avec camembert\n",
    "# Un peu long\n",
    "\n",
    "embedding = CamembertEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-09 10:16:33,614 Reading data from /home/sophie/Documents/Classification_Flair_Camembert\n",
      "2020-07-09 10:16:33,614 Train: /home/sophie/Documents/Classification_Flair_Camembert/train.csv\n",
      "2020-07-09 10:16:33,614 Dev: /home/sophie/Documents/Classification_Flair_Camembert/dev.csv\n",
      "2020-07-09 10:16:33,614 Test: /home/sophie/Documents/Classification_Flair_Camembert/test.csv\n",
      "{\n",
      "    \"TRAIN\": {\n",
      "        \"dataset\": \"TRAIN\",\n",
      "        \"total_number_of_documents\": 1781,\n",
      "        \"number_of_documents_per_class\": {\n",
      "            \"0\": 1416,\n",
      "            \"1\": 365\n",
      "        },\n",
      "        \"number_of_tokens_per_tag\": {},\n",
      "        \"number_of_tokens\": {\n",
      "            \"total\": 55574,\n",
      "            \"min\": 1,\n",
      "            \"max\": 56,\n",
      "            \"avg\": 31.20381807973049\n",
      "        }\n",
      "    },\n",
      "    \"TEST\": {\n",
      "        \"dataset\": \"TEST\",\n",
      "        \"total_number_of_documents\": 624,\n",
      "        \"number_of_documents_per_class\": {\n",
      "            \"1\": 34,\n",
      "            \"0\": 590\n",
      "        },\n",
      "        \"number_of_tokens_per_tag\": {},\n",
      "        \"number_of_tokens\": {\n",
      "            \"total\": 19728,\n",
      "            \"min\": 11,\n",
      "            \"max\": 56,\n",
      "            \"avg\": 31.615384615384617\n",
      "        }\n",
      "    },\n",
      "    \"DEV\": {\n",
      "        \"dataset\": \"DEV\",\n",
      "        \"total_number_of_documents\": 764,\n",
      "        \"number_of_documents_per_class\": {\n",
      "            \"1\": 137,\n",
      "            \"0\": 627\n",
      "        },\n",
      "        \"number_of_tokens_per_tag\": {},\n",
      "        \"number_of_tokens\": {\n",
      "            \"total\": 23691,\n",
      "            \"min\": 1,\n",
      "            \"max\": 50,\n",
      "            \"avg\": 31.009162303664922\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "## Corpus est l'outil fournis par FLAIR pour charger les jeux de données\n",
    "\n",
    "# 1. get the corpus\n",
    "path = os.getcwd()\n",
    "\n",
    "# this is the folder in which train, test and dev files reside\n",
    "data_folder = path = os.getcwd()\n",
    "\n",
    "# column format indicating which columns hold the text and label(s)\n",
    "column_name_map = {2: \"text\", 3: \"label_topic\"}\n",
    "\n",
    "# load corpus containing training, test and dev data and if CSV has a header, you can skip it\n",
    "corpus: Corpus = CSVClassificationCorpus(data_folder,                                        \n",
    "                                         column_name_map,\n",
    "                                         skip_header=True,\n",
    "                                         delimiter=';') \n",
    "\n",
    "# pour avoir une idée de à quoi ressemble les jeux de données, ça permet aussi de vérifier qu'on \n",
    "# ne s'est pas emmêler entre test et dev.\n",
    "\n",
    "import flair.datasets \n",
    "stats = corpus.obtain_statistics()\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-09 10:17:34,898 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2405/2405 [00:00<00:00, 4541.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-09 10:17:35,548 [b'0', b'1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. create the label dictionary\n",
    "label_dict = corpus.make_label_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. make a list of word embeddings\n",
    "word_embeddings = [embedding]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import DocumentRNNEmbeddings\n",
    "\n",
    "# 4. initialize document embedding by passing list of word embeddings\n",
    "# Can choose between many RNN types (GRU by default, to change use rnn_type parameter)\n",
    "document_embeddings: DocumentRNNEmbeddings = DocumentRNNEmbeddings(word_embeddings,\n",
    "                                                                   hidden_size=512,\n",
    "                                                                   reproject_words=True,\n",
    "                                                                   reproject_words_dimension=256,\n",
    "                                                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.models import TextClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. create the text classifier\n",
    "classifier = TextClassifier(document_embeddings, label_dictionary=label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.trainers import ModelTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. initialize the text classifier trainer\n",
    "trainer = ModelTrainer(classifier, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-09 10:19:16,511 ----------------------------------------------------------------------------------------------------\n",
      "2020-07-09 10:19:16,513 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): CamembertEmbeddings(\n",
      "        (model): CamembertModel(\n",
      "          (embeddings): RobertaEmbeddings(\n",
      "            (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
      "            (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "            (token_type_embeddings): Embedding(1, 768)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (encoder): BertEncoder(\n",
      "            (layer): ModuleList(\n",
      "              (0): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (1): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (2): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (3): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (4): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (5): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (6): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (7): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (8): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (9): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (10): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (11): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (pooler): BertPooler(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (activation): Tanh()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=768, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512, batch_first=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      "  (beta): 1.0\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-09 10:19:16,513 ----------------------------------------------------------------------------------------------------\n",
      "2020-07-09 10:19:16,514 Corpus: \"Corpus: 1781 train + 764 dev + 624 test sentences\"\n",
      "2020-07-09 10:19:16,514 ----------------------------------------------------------------------------------------------------\n",
      "2020-07-09 10:19:16,514 Parameters:\n",
      "2020-07-09 10:19:16,515  - learning_rate: \"0.2\"\n",
      "2020-07-09 10:19:16,515  - mini_batch_size: \"16\"\n",
      "2020-07-09 10:19:16,515  - patience: \"5\"\n",
      "2020-07-09 10:19:16,515  - anneal_factor: \"0.5\"\n",
      "2020-07-09 10:19:16,516  - max_epochs: \"10\"\n",
      "2020-07-09 10:19:16,516  - shuffle: \"True\"\n",
      "2020-07-09 10:19:16,516  - train_with_dev: \"False\"\n",
      "2020-07-09 10:19:16,517  - batch_growth_annealing: \"False\"\n",
      "2020-07-09 10:19:16,517 ----------------------------------------------------------------------------------------------------\n",
      "2020-07-09 10:19:16,517 Model training base path: \"resources/taggers/ag_news\"\n",
      "2020-07-09 10:19:16,518 ----------------------------------------------------------------------------------------------------\n",
      "2020-07-09 10:19:16,518 Device: cuda:0\n",
      "2020-07-09 10:19:16,518 ----------------------------------------------------------------------------------------------------\n",
      "2020-07-09 10:19:16,518 Embeddings storage mode: gpu\n",
      "2020-07-09 10:19:16,526 ----------------------------------------------------------------------------------------------------\n",
      "2020-07-09 10:19:18,419 epoch 1 - iter 11/112 - loss 0.68943411 - samples/sec: 98.78\n",
      "2020-07-09 10:19:20,325 epoch 1 - iter 22/112 - loss 0.58575240 - samples/sec: 98.14\n",
      "2020-07-09 10:19:22,161 epoch 1 - iter 33/112 - loss 0.58498329 - samples/sec: 101.28\n",
      "2020-07-09 10:19:24,015 epoch 1 - iter 44/112 - loss 0.58311197 - samples/sec: 104.51\n",
      "2020-07-09 10:19:25,803 epoch 1 - iter 55/112 - loss 0.56657282 - samples/sec: 104.58\n",
      "2020-07-09 10:19:27,569 epoch 1 - iter 66/112 - loss 0.55821677 - samples/sec: 108.97\n",
      "2020-07-09 10:19:29,299 epoch 1 - iter 77/112 - loss 0.54887808 - samples/sec: 107.77\n",
      "2020-07-09 10:19:31,108 epoch 1 - iter 88/112 - loss 0.55391483 - samples/sec: 102.60\n",
      "2020-07-09 10:19:32,928 epoch 1 - iter 99/112 - loss 0.53846261 - samples/sec: 105.88\n",
      "2020-07-09 10:19:34,754 epoch 1 - iter 110/112 - loss 0.53831515 - samples/sec: 101.71\n",
      "2020-07-09 10:19:35,068 ----------------------------------------------------------------------------------------------------\n",
      "2020-07-09 10:19:35,069 EPOCH 1 done: loss 0.5403 - lr 0.2000000\n",
      "2020-07-09 10:19:42,304 DEV : loss 0.6084395051002502 - score 0.7631\n",
      "2020-07-09 10:19:42,536 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-07-09 10:19:43,894 ----------------------------------------------------------------------------------------------------\n",
      "2020-07-09 10:19:45,888 epoch 2 - iter 11/112 - loss 0.57318311 - samples/sec: 99.62\n",
      "2020-07-09 10:19:47,631 epoch 2 - iter 22/112 - loss 0.57733612 - samples/sec: 107.21\n",
      "2020-07-09 10:19:49,403 epoch 2 - iter 33/112 - loss 0.54522825 - samples/sec: 108.27\n",
      "2020-07-09 10:19:51,199 epoch 2 - iter 44/112 - loss 0.53122442 - samples/sec: 103.93\n",
      "2020-07-09 10:19:52,989 epoch 2 - iter 55/112 - loss 0.51845519 - samples/sec: 103.86\n",
      "2020-07-09 10:19:54,831 epoch 2 - iter 66/112 - loss 0.50430962 - samples/sec: 104.43\n",
      "2020-07-09 10:19:56,636 epoch 2 - iter 77/112 - loss 0.50038904 - samples/sec: 102.61\n",
      "2020-07-09 10:19:58,388 epoch 2 - iter 88/112 - loss 0.50889007 - samples/sec: 106.32\n",
      "2020-07-09 10:20:00,217 epoch 2 - iter 99/112 - loss 0.51588953 - samples/sec: 105.38\n",
      "2020-07-09 10:20:02,003 epoch 2 - iter 110/112 - loss 0.50792262 - samples/sec: 104.11\n",
      "2020-07-09 10:20:02,337 ----------------------------------------------------------------------------------------------------\n",
      "2020-07-09 10:20:02,338 EPOCH 2 done: loss 0.5039 - lr 0.2000000\n",
      "2020-07-09 10:20:10,053 DEV : loss 0.4857785701751709 - score 0.8207\n",
      "2020-07-09 10:20:10,275 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-07-09 10:20:11,726 ----------------------------------------------------------------------------------------------------\n",
      "2020-07-09 10:20:13,647 epoch 3 - iter 11/112 - loss 0.48457390 - samples/sec: 97.88\n",
      "2020-07-09 10:20:15,581 epoch 3 - iter 22/112 - loss 0.48735390 - samples/sec: 95.99\n",
      "2020-07-09 10:20:17,440 epoch 3 - iter 33/112 - loss 0.50481373 - samples/sec: 100.02\n",
      "2020-07-09 10:20:19,192 epoch 3 - iter 44/112 - loss 0.51642897 - samples/sec: 106.20\n",
      "2020-07-09 10:20:21,009 epoch 3 - iter 55/112 - loss 0.51523119 - samples/sec: 105.74\n",
      "2020-07-09 10:20:22,841 epoch 3 - iter 66/112 - loss 0.50657735 - samples/sec: 101.63\n",
      "2020-07-09 10:20:24,697 epoch 3 - iter 77/112 - loss 0.50680427 - samples/sec: 103.48\n",
      "2020-07-09 10:20:26,519 epoch 3 - iter 88/112 - loss 0.49514656 - samples/sec: 101.98\n",
      "2020-07-09 10:20:28,280 epoch 3 - iter 99/112 - loss 0.50540895 - samples/sec: 105.99\n",
      "2020-07-09 10:20:30,075 epoch 3 - iter 110/112 - loss 0.49824045 - samples/sec: 107.83\n",
      "2020-07-09 10:20:30,400 ----------------------------------------------------------------------------------------------------\n",
      "2020-07-09 10:20:30,401 EPOCH 3 done: loss 0.4964 - lr 0.2000000\n",
      "2020-07-09 10:20:38,004 DEV : loss 0.4684421718120575 - score 0.8207\n",
      "2020-07-09 10:20:38,229 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-07-09 10:20:39,156 ----------------------------------------------------------------------------------------------------\n",
      "2020-07-09 10:20:41,048 epoch 4 - iter 11/112 - loss 0.41503943 - samples/sec: 103.87\n",
      "2020-07-09 10:20:42,903 epoch 4 - iter 22/112 - loss 0.45650636 - samples/sec: 100.13\n",
      "2020-07-09 10:20:44,755 epoch 4 - iter 33/112 - loss 0.48968864 - samples/sec: 100.68\n",
      "2020-07-09 10:20:46,657 epoch 4 - iter 44/112 - loss 0.48816184 - samples/sec: 100.74\n",
      "2020-07-09 10:20:48,442 epoch 4 - iter 55/112 - loss 0.48590950 - samples/sec: 104.28\n",
      "2020-07-09 10:20:50,190 epoch 4 - iter 66/112 - loss 0.48381018 - samples/sec: 106.71\n",
      "2020-07-09 10:20:51,989 epoch 4 - iter 77/112 - loss 0.47504184 - samples/sec: 103.53\n",
      "2020-07-09 10:20:53,829 epoch 4 - iter 88/112 - loss 0.47664552 - samples/sec: 101.06\n",
      "2020-07-09 10:20:55,687 epoch 4 - iter 99/112 - loss 0.47997383 - samples/sec: 100.08\n",
      "2020-07-09 10:20:57,586 epoch 4 - iter 110/112 - loss 0.48219522 - samples/sec: 101.24\n",
      "2020-07-09 10:20:57,927 ----------------------------------------------------------------------------------------------------\n",
      "2020-07-09 10:20:57,928 EPOCH 4 done: loss 0.4778 - lr 0.2000000\n",
      "2020-07-09 10:21:05,284 DEV : loss 0.5038332343101501 - score 0.8207\n",
      "2020-07-09 10:21:05,514 BAD EPOCHS (no improvement): 1\n",
      "2020-07-09 10:21:05,514 ----------------------------------------------------------------------------------------------------\n",
      "2020-07-09 10:21:07,454 epoch 5 - iter 11/112 - loss 0.47564193 - samples/sec: 101.18\n",
      "2020-07-09 10:21:09,308 epoch 5 - iter 22/112 - loss 0.50266683 - samples/sec: 100.42\n",
      "2020-07-09 10:21:11,070 epoch 5 - iter 33/112 - loss 0.50255282 - samples/sec: 105.86\n",
      "2020-07-09 10:21:12,894 epoch 5 - iter 44/112 - loss 0.50113395 - samples/sec: 105.45\n",
      "2020-07-09 10:21:14,716 epoch 5 - iter 55/112 - loss 0.51934329 - samples/sec: 101.92\n",
      "2020-07-09 10:21:16,604 epoch 5 - iter 66/112 - loss 0.50386620 - samples/sec: 102.04\n",
      "2020-07-09 10:21:18,420 epoch 5 - iter 77/112 - loss 0.49266961 - samples/sec: 102.18\n",
      "2020-07-09 10:21:20,139 epoch 5 - iter 88/112 - loss 0.49953247 - samples/sec: 108.49\n",
      "2020-07-09 10:21:21,929 epoch 5 - iter 99/112 - loss 0.48666352 - samples/sec: 107.64\n",
      "2020-07-09 10:21:23,694 epoch 5 - iter 110/112 - loss 0.48010953 - samples/sec: 105.20\n",
      "2020-07-09 10:21:24,033 ----------------------------------------------------------------------------------------------------\n",
      "2020-07-09 10:21:24,033 EPOCH 5 done: loss 0.4848 - lr 0.2000000\n",
      "2020-07-09 10:21:31,478 DEV : loss 1.449820876121521 - score 0.2016\n",
      "2020-07-09 10:21:31,703 BAD EPOCHS (no improvement): 2\n",
      "2020-07-09 10:21:31,704 ----------------------------------------------------------------------------------------------------\n",
      "2020-07-09 10:21:33,442 epoch 6 - iter 11/112 - loss 0.53130351 - samples/sec: 108.17\n",
      "2020-07-09 10:21:35,236 epoch 6 - iter 22/112 - loss 0.48772226 - samples/sec: 103.64\n",
      "2020-07-09 10:21:37,220 epoch 6 - iter 33/112 - loss 0.46669049 - samples/sec: 97.90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-09 10:21:38,979 epoch 6 - iter 44/112 - loss 0.50774995 - samples/sec: 105.82\n",
      "2020-07-09 10:21:40,831 epoch 6 - iter 55/112 - loss 0.49121912 - samples/sec: 100.67\n",
      "2020-07-09 10:21:42,659 epoch 6 - iter 66/112 - loss 0.49028140 - samples/sec: 105.27\n",
      "2020-07-09 10:21:44,480 epoch 6 - iter 77/112 - loss 0.49536724 - samples/sec: 102.32\n",
      "2020-07-09 10:21:46,416 epoch 6 - iter 88/112 - loss 0.49131831 - samples/sec: 99.70\n",
      "2020-07-09 10:21:48,310 epoch 6 - iter 99/112 - loss 0.48387189 - samples/sec: 97.47\n",
      "2020-07-09 10:21:50,144 epoch 6 - iter 110/112 - loss 0.48473317 - samples/sec: 101.45\n",
      "2020-07-09 10:21:50,468 ----------------------------------------------------------------------------------------------------\n",
      "2020-07-09 10:21:50,469 EPOCH 6 done: loss 0.4843 - lr 0.2000000\n",
      "2020-07-09 10:21:58,225 DEV : loss 0.4095252454280853 - score 0.8207\n",
      "2020-07-09 10:21:58,456 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-07-09 10:21:59,396 ----------------------------------------------------------------------------------------------------\n",
      "2020-07-09 10:22:01,256 epoch 7 - iter 11/112 - loss 0.49960920 - samples/sec: 102.25\n",
      "2020-07-09 10:22:03,126 epoch 7 - iter 22/112 - loss 0.48206118 - samples/sec: 103.81\n",
      "2020-07-09 10:22:04,917 epoch 7 - iter 33/112 - loss 0.44076441 - samples/sec: 104.06\n",
      "2020-07-09 10:22:06,659 epoch 7 - iter 44/112 - loss 0.45661245 - samples/sec: 106.77\n",
      "2020-07-09 10:22:08,511 epoch 7 - iter 55/112 - loss 0.45436003 - samples/sec: 100.31\n",
      "2020-07-09 10:22:10,393 epoch 7 - iter 66/112 - loss 0.45818757 - samples/sec: 102.46\n",
      "2020-07-09 10:22:12,194 epoch 7 - iter 77/112 - loss 0.46647261 - samples/sec: 103.77\n",
      "2020-07-09 10:22:14,032 epoch 7 - iter 88/112 - loss 0.45926474 - samples/sec: 104.34\n",
      "2020-07-09 10:22:15,946 epoch 7 - iter 99/112 - loss 0.46101183 - samples/sec: 97.33\n",
      "2020-07-09 10:22:17,776 epoch 7 - iter 110/112 - loss 0.46196775 - samples/sec: 101.52\n",
      "2020-07-09 10:22:18,110 ----------------------------------------------------------------------------------------------------\n",
      "2020-07-09 10:22:18,111 EPOCH 7 done: loss 0.4653 - lr 0.2000000\n",
      "2020-07-09 10:22:26,069 DEV : loss 0.4399270713329315 - score 0.822\n",
      "2020-07-09 10:22:26,297 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-07-09 10:22:27,208 ----------------------------------------------------------------------------------------------------\n",
      "2020-07-09 10:22:29,173 epoch 8 - iter 11/112 - loss 0.30560868 - samples/sec: 100.26\n",
      "2020-07-09 10:22:31,049 epoch 8 - iter 22/112 - loss 0.44361387 - samples/sec: 99.13\n",
      "2020-07-09 10:22:32,859 epoch 8 - iter 33/112 - loss 0.43137320 - samples/sec: 102.80\n",
      "2020-07-09 10:22:34,823 epoch 8 - iter 44/112 - loss 0.43268706 - samples/sec: 98.43\n",
      "2020-07-09 10:22:36,644 epoch 8 - iter 55/112 - loss 0.44855715 - samples/sec: 102.05\n",
      "2020-07-09 10:22:38,502 epoch 8 - iter 66/112 - loss 0.44259586 - samples/sec: 100.00\n",
      "2020-07-09 10:22:40,384 epoch 8 - iter 77/112 - loss 0.45377741 - samples/sec: 102.24\n",
      "2020-07-09 10:22:42,179 epoch 8 - iter 88/112 - loss 0.45816640 - samples/sec: 103.75\n",
      "2020-07-09 10:22:43,913 epoch 8 - iter 99/112 - loss 0.45182667 - samples/sec: 107.61\n",
      "2020-07-09 10:22:45,682 epoch 8 - iter 110/112 - loss 0.45467141 - samples/sec: 108.79\n",
      "2020-07-09 10:22:45,999 ----------------------------------------------------------------------------------------------------\n",
      "2020-07-09 10:22:46,000 EPOCH 8 done: loss 0.4522 - lr 0.2000000\n",
      "2020-07-09 10:22:53,531 DEV : loss 0.3870841860771179 - score 0.8377\n",
      "2020-07-09 10:22:53,753 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-07-09 10:22:54,657 ----------------------------------------------------------------------------------------------------\n",
      "2020-07-09 10:22:56,515 epoch 9 - iter 11/112 - loss 0.45997836 - samples/sec: 106.08\n",
      "2020-07-09 10:22:58,326 epoch 9 - iter 22/112 - loss 0.43828137 - samples/sec: 102.84\n",
      "2020-07-09 10:23:00,139 epoch 9 - iter 33/112 - loss 0.47342458 - samples/sec: 102.88\n",
      "2020-07-09 10:23:02,046 epoch 9 - iter 44/112 - loss 0.46100178 - samples/sec: 97.35\n",
      "2020-07-09 10:23:03,838 epoch 9 - iter 55/112 - loss 0.45150071 - samples/sec: 103.89\n",
      "2020-07-09 10:23:05,643 epoch 9 - iter 66/112 - loss 0.44594719 - samples/sec: 106.54\n",
      "2020-07-09 10:23:07,411 epoch 9 - iter 77/112 - loss 0.43504967 - samples/sec: 105.42\n",
      "2020-07-09 10:23:09,244 epoch 9 - iter 88/112 - loss 0.43143541 - samples/sec: 101.71\n",
      "2020-07-09 10:23:11,151 epoch 9 - iter 99/112 - loss 0.43180142 - samples/sec: 100.61\n",
      "2020-07-09 10:23:12,923 epoch 9 - iter 110/112 - loss 0.42882464 - samples/sec: 104.90\n",
      "2020-07-09 10:23:13,251 ----------------------------------------------------------------------------------------------------\n",
      "2020-07-09 10:23:13,252 EPOCH 9 done: loss 0.4240 - lr 0.2000000\n",
      "2020-07-09 10:23:20,840 DEV : loss 0.49985700845718384 - score 0.8207\n",
      "2020-07-09 10:23:21,061 BAD EPOCHS (no improvement): 1\n",
      "2020-07-09 10:23:21,061 ----------------------------------------------------------------------------------------------------\n",
      "2020-07-09 10:23:22,966 epoch 10 - iter 11/112 - loss 0.42116058 - samples/sec: 98.43\n",
      "2020-07-09 10:23:24,843 epoch 10 - iter 22/112 - loss 0.40995401 - samples/sec: 103.82\n",
      "2020-07-09 10:23:26,572 epoch 10 - iter 33/112 - loss 0.39583773 - samples/sec: 107.58\n",
      "2020-07-09 10:23:28,373 epoch 10 - iter 44/112 - loss 0.41900174 - samples/sec: 103.32\n",
      "2020-07-09 10:23:30,282 epoch 10 - iter 55/112 - loss 0.41884392 - samples/sec: 100.72\n",
      "2020-07-09 10:23:32,096 epoch 10 - iter 66/112 - loss 0.41982160 - samples/sec: 102.67\n",
      "2020-07-09 10:23:33,886 epoch 10 - iter 77/112 - loss 0.41541661 - samples/sec: 103.86\n",
      "2020-07-09 10:23:35,795 epoch 10 - iter 88/112 - loss 0.41393424 - samples/sec: 97.33\n",
      "2020-07-09 10:23:37,627 epoch 10 - iter 99/112 - loss 0.41349198 - samples/sec: 101.63\n",
      "2020-07-09 10:23:39,425 epoch 10 - iter 110/112 - loss 0.41808701 - samples/sec: 106.74\n",
      "2020-07-09 10:23:39,739 ----------------------------------------------------------------------------------------------------\n",
      "2020-07-09 10:23:39,740 EPOCH 10 done: loss 0.4174 - lr 0.2000000\n",
      "2020-07-09 10:23:47,047 DEV : loss 0.38226747512817383 - score 0.8259\n",
      "2020-07-09 10:23:47,269 BAD EPOCHS (no improvement): 2\n",
      "2020-07-09 10:23:48,121 ----------------------------------------------------------------------------------------------------\n",
      "2020-07-09 10:23:48,123 Testing using best model ...\n",
      "2020-07-09 10:23:48,126 loading file resources/taggers/ag_news/best-model.pt\n",
      "2020-07-09 10:24:04,380 0.9455128205128205\t0.9455128205128205\t0.9455128205128205\n",
      "2020-07-09 10:24:04,381 \n",
      "MICRO_AVG: acc 0.9455128205128205 - f1-score 0.9455128205128205\n",
      "MACRO_AVG: acc 0.9455128205128205 - f1-score 0.59951676230746\n",
      "0          tp: 585 - fp: 29 - fn: 5 - tn: 5 - precision: 0.9528 - recall: 0.9915 - accuracy: 0.9455 - f1-score: 0.9718\n",
      "1          tp: 5 - fp: 5 - fn: 29 - tn: 585 - precision: 0.5000 - recall: 0.1471 - accuracy: 0.9455 - f1-score: 0.2273\n",
      "2020-07-09 10:24:04,381 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_score': 0.9455128205128205,\n",
       " 'dev_score_history': [0.7630890052356021,\n",
       "  0.8206806282722513,\n",
       "  0.8206806282722513,\n",
       "  0.8206806282722513,\n",
       "  0.20157068062827224,\n",
       "  0.8206806282722513,\n",
       "  0.8219895287958116,\n",
       "  0.837696335078534,\n",
       "  0.8206806282722513,\n",
       "  0.8259162303664922],\n",
       " 'train_loss_history': [0.5402974475707326,\n",
       "  0.5038986390988741,\n",
       "  0.49641720338591505,\n",
       "  0.47775933645399554,\n",
       "  0.48482047686619417,\n",
       "  0.48428113672084044,\n",
       "  0.46529376287279384,\n",
       "  0.452166155845459,\n",
       "  0.42398965052728144,\n",
       "  0.41737897374800276],\n",
       " 'dev_loss_history': [0.6084395051002502,\n",
       "  0.4857785701751709,\n",
       "  0.4684421718120575,\n",
       "  0.5038332343101501,\n",
       "  1.449820876121521,\n",
       "  0.4095252454280853,\n",
       "  0.4399270713329315,\n",
       "  0.3870841860771179,\n",
       "  0.49985700845718384,\n",
       "  0.38226747512817383]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. start the training\n",
    "trainer.train('resources/taggers/ag_news',\n",
    "              learning_rate=0.2,\n",
    "              mini_batch_size=16,\n",
    "              anneal_factor=0.5,\n",
    "              patience=5,\n",
    "              max_epochs=10, # augmenter le nombre d'époch\n",
    "              embeddings_storage_mode='gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pour fine tuner le modèle, Flair propose une 'grid search' des hyperparamètres\n",
    "\n",
    "from flair.hyperparameter.param_selection import TextClassifierParamSelector, OptimizationValue\n",
    "from hyperopt import hp\n",
    "from flair.hyperparameter.param_selection import SearchSpace, Parameter\n",
    "from flair.data_fetcher import NLPTaskDataFetcher\n",
    "from pathlib import Path\n",
    "from flair.embeddings import StackedEmbeddings\n",
    "\n",
    "# define your search space\n",
    "search_space = SearchSpace()\n",
    "\n",
    "search_space.add(Parameter.EMBEDDINGS, hp.choice, options=[[ embedding ]])\n",
    "search_space.add(Parameter.LEARNING_RATE, hp.choice, options=[0.05, 0.1, 0.15, 0.2])\n",
    "search_space.add(Parameter.MINI_BATCH_SIZE, hp.choice, options=[8, 16, 32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lancer le selecteur d'hyperparamètres\n",
    "\n",
    "param_selector = TextClassifierParamSelector(\n",
    "    corpus=corpus, \n",
    "    multi_label=False, \n",
    "    base_path='resources/results',\n",
    "    document_embedding_type='rnn',\n",
    "    max_epochs=10, \n",
    "    training_runs=1,\n",
    "    optimization_value=OptimizationValue.DEV_SCORE\n",
    ")\n",
    "\n",
    "param_selector.optimize(search_space, max_evals=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-09 10:24:15,429 loading file resources/taggers/ag_news/final-model.pt\n",
      "[0 (0.5260)]\n"
     ]
    }
   ],
   "source": [
    "# Once the model is trained you can load it to predict the class of new sentences. \n",
    "\n",
    "# Just call the predict method of the model.\n",
    "\n",
    "classifier = TextClassifier.load('resources/taggers/ag_news/final-model.pt')\n",
    "\n",
    "\n",
    "# create example sentence\n",
    "\n",
    "sentence = Sentence('complication precoce : ACR en salle de catheterimse reanime par adrenaline et massage cardiaque no flow 1 min low flow 5 min')\n",
    "\n",
    "# predict class and print\n",
    "\n",
    "classifier.predict(sentence)\n",
    "\n",
    "print(sentence.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résultat pour le test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predire le label des exemples dans le test set\n",
    "\n",
    "y_pred = []\n",
    "for row in test_set.iterrows():\n",
    "    #print(\"Train example:\", row[1].exemple)\n",
    "    sentence = Sentence(row[1].exemple)\n",
    "    classifier.predict(sentence)\n",
    "    #print(\"Predicted class\", sentence.labels)\n",
    "    y_pred.append(sentence.labels)\n",
    "    \n",
    "y_pred = pd.DataFrame(y_pred)\n",
    "df = y_pred.columns=['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = y_pred['result'].astype(str).str[:1].astype(int)\n",
    "\n",
    "y_pred = df.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test_set[\"label\"]\n",
    "\n",
    "y_true = y_true.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global F1-score\n",
      "0.9150641025641025\n",
      "F1-score par classe\n",
      "[0.95473954 0.31168831]\n",
      "Recall global\n",
      "0.9150641025641025\n",
      "Recall par classe\n",
      "[0.94745763 0.35294118]\n",
      "Precision global\n",
      "0.9150641025641025\n",
      "Precision par classe\n",
      "[0.96213425 0.27906977]\n",
      "Accuracy\n",
      "0.9150641025641025\n",
      "Average precision-recall score: 0.13\n"
     ]
    }
   ],
   "source": [
    "# Metriques\n",
    "\n",
    "F1 = f1_score(y_true, y_pred, average='micro')\n",
    "F2 = f1_score(y_true, y_pred, average=None)\n",
    "RC1 = recall_score(y_true, y_pred, average='micro')\n",
    "RC2 = recall_score(y_true, y_pred, average=None)\n",
    "PC1 = precision_score(y_true, y_pred, average='micro')\n",
    "PC2 = precision_score(y_true, y_pred, average=None)\n",
    "AC = accuracy_score(y_true, y_pred)\n",
    "\n",
    "print('global F1-score')\n",
    "print(F1)\n",
    "print('F1-score par classe')\n",
    "print(F2)\n",
    "print('Recall global')\n",
    "print(RC1)\n",
    "print('Recall par classe')\n",
    "print(RC2)\n",
    "print('Precision global')\n",
    "print(PC1)\n",
    "print('Precision par classe')\n",
    "print(PC2)\n",
    "print('Accuracy')\n",
    "print(AC)\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "average_precision = average_precision_score(y_true, y_pred)\n",
    "print('Average precision-recall score: {0:0.2f}'.format(\n",
    "      average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice\n",
    "\n",
    "CM = confusion_matrix(y_true, y_pred, labels=[1, 0])\n",
    "\n",
    "array = CM\n",
    "\n",
    "labels=[1, 0]\n",
    "\n",
    "\n",
    "print(CM)\n",
    "\n",
    "df_cm = pd.DataFrame(array, index=labels, columns=labels)\n",
    "sn.set(font_scale=1.4)\n",
    "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}, cmap=\"YlGnBu\") # font size\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résultats pour le dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenir la liste des predictions dans le dev\n",
    "\n",
    "dev_pred = []\n",
    "for row in dev_set.iterrows():\n",
    "    #print(\"Train example:\", row[1].exemple)\n",
    "    sentence = Sentence(row[1].exemple)\n",
    "    classifier.predict(sentence)\n",
    "    #print(\"Predicted class\", sentence.labels)\n",
    "    dev_pred.append(sentence.labels)\n",
    "    \n",
    "dev_pred = pd.DataFrame(dev_pred)\n",
    "df = dev_pred.columns=['result']\n",
    "df = dev_pred['result'].astype(str).str[:1].astype(int)\n",
    "dev_pred = df.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenir la liste des vrais labels dans le dev\n",
    "dev_true = dev_set[\"label\"]\n",
    "dev_true = dev_true.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global F1-score\n",
      "0.912303664921466\n",
      "F1-score par classe\n",
      "[0.94678316 0.75092937]\n",
      "Confusion matrix\n",
      "[[101  36]\n",
      " [ 31 596]]\n",
      "Recall global\n",
      "0.912303664921466\n",
      "Recall par classe\n",
      "[0.95055821 0.73722628]\n",
      "Precision global\n",
      "0.912303664921466\n",
      "Precision par classe\n",
      "[0.94303797 0.76515152]\n",
      "Accuracy\n",
      "0.912303664921466\n",
      "Average precision-recall score: 0.61\n"
     ]
    }
   ],
   "source": [
    "# Calculs des métriques pour le dev\n",
    "\n",
    "F1 = f1_score(dev_true, dev_pred, average='micro')\n",
    "F2 = f1_score(dev_true, dev_pred, average=None)\n",
    "RC1 = recall_score(dev_true, dev_pred, average='micro')\n",
    "RC2 = recall_score(dev_true, dev_pred, average=None)\n",
    "PC1 = precision_score(dev_true, dev_pred, average='micro')\n",
    "PC2 = precision_score(dev_true, dev_pred, average=None)\n",
    "AC = accuracy_score(dev_true, dev_pred)\n",
    "CM = confusion_matrix(dev_true, dev_pred, labels=[1, 0])\n",
    "\n",
    "\n",
    "print('global F1-score')\n",
    "print(F1)\n",
    "print('F1-score par classe')\n",
    "print(F2)\n",
    "print('Confusion matrix')\n",
    "print(CM)\n",
    "print('Recall global')\n",
    "print(RC1)\n",
    "print('Recall par classe')\n",
    "print(RC2)\n",
    "print('Precision global')\n",
    "print(PC1)\n",
    "print('Precision par classe')\n",
    "print(PC2)\n",
    "print('Accuracy')\n",
    "print(AC)\n",
    "\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "average_precision = average_precision_score(dev_true, dev_pred)\n",
    "print('Average precision-recall score: {0:0.2f}'.format(\n",
    "      average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de confusion pour le dev\n",
    "\n",
    "CM = confusion_matrix(dev_true, dev_pred, labels=[1, 0])\n",
    "\n",
    "array = CM\n",
    "\n",
    "labels=[1, 0]\n",
    "\n",
    "df_cm = pd.DataFrame(array, index=labels, columns=labels)\n",
    "sn.set(font_scale=1.4)\n",
    "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}, cmap=\"YlGn\") # font size\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIFICATION DES SEQUENCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlab_set = pd.read_csv('unlab.csv', sep =';', encoding = \"utf-8\").set_index('Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification des unlabeled\n",
    "\n",
    "unlab = []\n",
    "for row in unlab_set.iterrows():\n",
    "    #print(\"Train example:\", row[1].exemple)\n",
    "    sentence = Sentence(row[1].texte_complication)\n",
    "    classifier.predict(sentence)\n",
    "    #print(\"Predicted class\", sentence.labels)\n",
    "    unlab.append(sentence.labels)\n",
    "    \n",
    "unlab = pd.DataFrame(unlab)\n",
    "df = unlab.columns=['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlab.to_csv(\"unlab_classifier.csv\", sep=';', encoding = 'utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
