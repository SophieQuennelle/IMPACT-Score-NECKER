{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Samples Embedding with CamemBERT then samples classification by FLAIR TextClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import flair\n",
    "from flair.data import Sentence\n",
    "from flair.embeddings import CamembertEmbeddings\n",
    "from flair.embeddings import TransformerWordEmbeddings\n",
    "import pathlib\n",
    "import os\n",
    "from flair.data import Corpus\n",
    "from flair.datasets import CSVClassificationCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les jeux de données : test = jeu d'évaluation final pour flair, \n",
    "# Annoté sans active learning, parce que l'AL pourrait entraîner un biais.\n",
    "\n",
    "train_set = pd.read_csv('train.csv', sep=';', encoding='utf-8')\n",
    "dev_set = pd.read_csv('dev.csv', sep=';', encoding='utf-8')\n",
    "test_set = pd.read_csv('test.csv', sep=';', encoding='utf-8')\n",
    "\n",
    "train_set = train_set[['index', 'exemple', 'label']].set_index('index')\n",
    "test_set = test_set[['index', 'exemple', 'label']].set_index('index')\n",
    "dev_set = dev_set[['index', 'exemple', 'label']].set_index('index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Camembert Embedding\n",
    "\n",
    "embedding = CamembertEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus is a tool from Flair to load train and test set.\n",
    "\n",
    "# 1. get the corpus\n",
    "path = os.getcwd()\n",
    "\n",
    "# this is the folder in which train, test and dev files reside\n",
    "data_folder = path = os.getcwd()\n",
    "\n",
    "# column format indicating which columns hold the text and label(s)\n",
    "column_name_map = {2: \"text\", 3: \"label_topic\"}\n",
    "\n",
    "# load corpus containing training, test and dev data and if CSV has a header, you can skip it\n",
    "corpus: Corpus = CSVClassificationCorpus(data_folder,                                        \n",
    "                                         column_name_map,\n",
    "                                         skip_header=True,\n",
    "                                         delimiter=';') \n",
    "\n",
    "# look at what the datasets look like\n",
    "import flair.datasets \n",
    "stats = corpus.obtain_statistics()\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-12 11:36:42,610 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2830/2830 [00:00<00:00, 4365.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-12 11:36:43,367 [b'0', b'1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. create the label dictionary\n",
    "label_dict = corpus.make_label_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. make a list of word embeddings\n",
    "word_embeddings = [embedding]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import DocumentRNNEmbeddings\n",
    "\n",
    "# 4. initialize document embedding by passing list of word embeddings\n",
    "# Can choose between many RNN types (GRU by default, to change use rnn_type parameter)\n",
    "document_embeddings: DocumentRNNEmbeddings = DocumentRNNEmbeddings(word_embeddings,\n",
    "                                                                   hidden_size=512,\n",
    "                                                                   reproject_words=True,\n",
    "                                                                   reproject_words_dimension=256,\n",
    "                                                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.models import TextClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. create the text classifier\n",
    "classifier = TextClassifier(document_embeddings, label_dictionary=label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.trainers import ModelTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. initialize the text classifier trainer\n",
    "trainer = ModelTrainer(classifier, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. start the training\n",
    "trainer.train('resources/taggers/complication_kt',\n",
    "              learning_rate=0.2\n",
    "              mini_batch_size=16,\n",
    "              anneal_factor=0.5,\n",
    "              patience=5,\n",
    "              max_epochs=112,\n",
    "              embeddings_storage_mode='gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "from flair.visual.training_curves import Plotter\n",
    "plotter = Plotter()\n",
    "\n",
    "plotter.plot_training_curves('./resources/taggers/complication_kt/loss.tsv')\n",
    "\n",
    "plotter.plot_weights('./resources/taggers/complication_kt/weights.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once the model is trained you can load it to predict the class of new sentences. \n",
    "\n",
    "# Just call the predict method of the model.\n",
    "\n",
    "classifier = TextClassifier.load('resources/taggers/complication_kt/final-model.pt')\n",
    "\n",
    "\n",
    "# Create example sentence\n",
    "\n",
    "sentence = Sentence('complication precoce : ACR en salle de catheterimse reanime par adrenaline et massage cardiaque no flow 1 min low flow 5 min')\n",
    "\n",
    "# Predict class and print\n",
    "\n",
    "classifier.predict(sentence)\n",
    "\n",
    "print(sentence.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict samples label\n",
    "\n",
    "y_pred = []\n",
    "for row in test_set.iterrows():\n",
    "    #print(\"Train example:\", row[1].exemple)\n",
    "    sentence = Sentence(row[1].exemple)\n",
    "    classifier.predict(sentence)\n",
    "    #print(\"Predicted class\", sentence.labels)\n",
    "    y_pred.append(sentence.labels)\n",
    "    \n",
    "y_pred = pd.DataFrame(y_pred)\n",
    "df = y_pred.columns=['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = y_pred['result'].astype(str).str[:1].astype(int)\n",
    "\n",
    "y_pred = df.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test_set[\"label\"]\n",
    "\n",
    "y_true = y_true.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "\n",
    "F1 = f1_score(y_true, y_pred, average='micro')\n",
    "F2 = f1_score(y_true, y_pred, average=None)\n",
    "RC1 = recall_score(y_true, y_pred, average='micro')\n",
    "RC2 = recall_score(y_true, y_pred, average=None)\n",
    "PC1 = precision_score(y_true, y_pred, average='micro')\n",
    "PC2 = precision_score(y_true, y_pred, average=None)\n",
    "AC = accuracy_score(y_true, y_pred)\n",
    "\n",
    "print('global F1-score')\n",
    "print(F1)\n",
    "print('F1-score by class')\n",
    "print(F2)\n",
    "print('Recall global')\n",
    "print(RC1)\n",
    "print('Recall by class')\n",
    "print(RC2)\n",
    "print('Precision global')\n",
    "print(PC1)\n",
    "print('Precision by class')\n",
    "print(PC2)\n",
    "print('Accuracy')\n",
    "print(AC)\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "average_precision = average_precision_score(y_true, y_pred)\n",
    "print('Average precision-recall score: {0:0.2f}'.format(\n",
    "      average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice\n",
    "\n",
    "CM = confusion_matrix(y_true, y_pred, labels=[1, 0])\n",
    "\n",
    "array = CM\n",
    "\n",
    "labels=[1, 0]\n",
    "\n",
    "print(CM)\n",
    "\n",
    "fig = plt.gcf()\n",
    "df_cm = pd.DataFrame(array, index=labels, columns=labels)\n",
    "sn.set(font_scale=1.4)\n",
    "sn.heatmap(df_cm,  annot=True, annot_kws={\"size\": 16}, cmap=\"YlGnBu\", fmt='g') # font size\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.show()\n",
    "fig.savefig('CM_flair.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A table to look at results on test samples\n",
    "test = test_set.reset_index()\n",
    "resultats_test = test.join(df)\n",
    "resultats_test.to_csv(\"resultats_test.csv\", sep=';', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get patient wise result (on patients that are on test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To link correspondence table and results on test set\n",
    "\n",
    "# Cf table\n",
    "cf = pd.read_csv('table_de_correspondance.csv', \n",
    "                 sep=';', \n",
    "                 encoding='utf-8',\n",
    "                index_col=0)\n",
    "\n",
    "print(cf.IPP.nunique())\n",
    "\n",
    "# Result table\n",
    "res = pd.read_csv('resultats_test.csv', \n",
    "                 sep=';', \n",
    "                 encoding='utf-8')\n",
    "res['index'] = res['index'].str.extract('(\\d+)')\n",
    "res = res[['index', 'exemple', 'label', 'result']]\n",
    "res['index'] = res['index'].astype(int)\n",
    "res = res.set_index('index')\n",
    "\n",
    "# Left merge on result to get patient ID\n",
    "df = pd.merge(res, cf, how='left', left_index=True, right_index=True)\n",
    "\n",
    "print(df)\n",
    "print(df.IPP.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link every sample to its patient\n",
    "df.groupby(['IPP', 'label']).groups\n",
    "\n",
    "# We consider that if a patient had at least one sample positive he should be count as a complicated procedure \n",
    "compare = df.groupby(['IPP'])[\"result\", \"label\"].apply(lambda x : x.astype(int).sum())\n",
    "compare['label'][compare['label'] > 0] = 1\n",
    "compare['result'][compare['result'] > 0] = 1\n",
    "compare['erreur'] = compare['label'] + compare['result']\n",
    "\n",
    "# To have a look on compare table\n",
    "compare.to_csv('compare_par_IPP.csv', sep=\";\", encoding='utf-8')\n",
    "\n",
    "# erreur = 0 -> True negative, error = 2 -> True positive, error = 1 soit False negative or False positive.\n",
    "print(compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "\n",
    "yp_true = compare[\"label\"]\n",
    "yp_true = yp_true.tolist()\n",
    "\n",
    "yp_pred = compare[\"result\"]\n",
    "yp_pred = yp_pred.tolist()\n",
    "\n",
    "F1 = f1_score(yp_true, yp_pred, average='micro')\n",
    "F2 = f1_score(yp_true, yp_pred, average=None)\n",
    "RC1 = recall_score(yp_true, yp_pred, average='micro')\n",
    "RC2 = recall_score(yp_true, yp_pred, average=None)\n",
    "PC1 = precision_score(yp_true, yp_pred, average='micro')\n",
    "PC2 = precision_score(yp_true, yp_pred, average=None)\n",
    "AC = accuracy_score(yp_true, yp_pred)\n",
    "\n",
    "print('global F1-score')\n",
    "print(F1)\n",
    "print('F1-score par classe')\n",
    "print(F2)\n",
    "print('Recall global')\n",
    "print(RC1)\n",
    "print('Recall par classe')\n",
    "print(RC2)\n",
    "print('Precision global')\n",
    "print(PC1)\n",
    "print('Precision par classe')\n",
    "print(PC2)\n",
    "print('Accuracy')\n",
    "print(AC)\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "average_precision = average_precision_score(yp_true, yp_pred)\n",
    "print('Average precision-recall score: {0:0.2f}'.format(\n",
    "      average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice\n",
    "\n",
    "CM = confusion_matrix(yp_true, yp_pred, labels=[1, 0])\n",
    "\n",
    "array = CM\n",
    "\n",
    "labels=[1, 0]\n",
    "\n",
    "print(CM)\n",
    "\n",
    "fig = plt.gcf()\n",
    "df_cm = pd.DataFrame(array, index=labels, columns=labels)\n",
    "sn.set(font_scale=1.4)\n",
    "sn.heatmap(df_cm,  annot=True, annot_kws={\"size\": 16}, cmap=\"Oranges\", fmt='g') # font size\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.show()\n",
    "fig.savefig('CM_patient_flair.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIFICATION of UNLABELED SAMPLES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When Text classifier results are adequate. You can load unclassified samples and let the model gives them a label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load unlabeled samples\n",
    "\n",
    "unlab_set = pd.read_csv('unlab.csv', sep =';', encoding = \"utf-8\").set_index('Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify unlabeled\n",
    "\n",
    "unlab = []\n",
    "for row in unlab_set.iterrows():\n",
    "    #print(\"Train example:\", row[1].exemple)\n",
    "    sentence = Sentence(row[1].exemple)\n",
    "    classifier.predict(sentence)\n",
    "    #print(\"Predicted class\", sentence.labels)\n",
    "    unlab.append(sentence.labels)\n",
    "    \n",
    "unlab = pd.DataFrame(unlab)\n",
    "unlab = unlab_set.join(unlab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saved classified samples in a csv document that will be used for the score estimation\n",
    "\n",
    "unlab.to_csv(\"unlab_classifier.csv\", sep=';', encoding = 'utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
